{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "This Jupyter Notebook is complementary to the PDF report submitted. Please refer to the report for more explanations according to the requested information of the assignment. For the graded portions, you may jump straight to the [Actual Codes](#Actual-Codes) onwards.\n",
    "\n",
    "[Load Packages](#Load-Packages)<br>\n",
    "[Data Description](#Data-Description)<br>\n",
    "[Data Processing](#Data-Processing)<br>\n",
    "[Q1. Beta-binomial Naive Bayes](#Q1.-Beta-binomial-Naive-Bayes)\n",
    "* [Actual Codes](#Actual-Codes)<br>\n",
    "* [Q1 (a)](#Q1-(a))\n",
    "* [Q1 (b)](#Q1-(b))\n",
    "* [Q1 (c)](#Q1-(c))\n",
    "\n",
    "[Q2. Gaussian Naive Bayes](#Q2.-Gaussian-Naive-Bayes)\n",
    "* [Q2 (a)](#Q2-(a))\n",
    "\n",
    "[Q3. Logistic Regression](#Q3.-Logistic-Regression)\n",
    "* [Q3 (a)](#Q3-(a))\n",
    "* [Q3 (b)](#Q3-(b))\n",
    "* [Q3 (c)](#Q3-(c))\n",
    "\n",
    "[Q4. K-Nearest Neighbors](#Q4.-K-Nearest-Neighbors)<br>\n",
    "[Q5. Survey](#Q5.-Survey)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "The data is an email spam dataset, consisting of 4601 email messages with 57 features. Feature\n",
    "descriptions are found [in this link](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.info.txt). We have divided the data into a training set (3065 emails)\n",
    "and test set (1536 emails) with accompanying labels (1 = spam , 0 = not spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file, Platform: MACI64, Created on: Sun Aug  9 09:46:15 2020', '__version__': '1.0', '__globals__': [], 'Xtrain': array([[5.100e-01, 4.300e-01, 2.900e-01, ..., 6.518e+00, 6.110e+02,\n",
      "        2.340e+03],\n",
      "       [0.000e+00, 0.000e+00, 1.440e+00, ..., 2.380e+00, 8.000e+00,\n",
      "        5.000e+01],\n",
      "       [1.600e-01, 3.200e-01, 6.500e-01, ..., 6.586e+00, 1.320e+02,\n",
      "        9.550e+02],\n",
      "       ...,\n",
      "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 1.759e+00, 2.100e+01,\n",
      "        4.030e+02],\n",
      "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 2.333e+00, 1.000e+01,\n",
      "        4.900e+01],\n",
      "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 1.433e+00, 5.000e+00,\n",
      "        8.600e+01]]), 'Xtest': array([[  0.   ,   0.   ,   0.   , ...,   1.536,   8.   ,  63.   ],\n",
      "       [  0.   ,   0.   ,   0.   , ...,   1.25 ,   3.   ,  15.   ],\n",
      "       [  0.   ,   0.   ,   0.41 , ...,   1.522,  11.   ,  67.   ],\n",
      "       ...,\n",
      "       [  0.   ,   0.   ,   0.   , ...,   2.044,  22.   ,  92.   ],\n",
      "       [  0.   ,   0.   ,   0.   , ...,   2.   ,   3.   ,   6.   ],\n",
      "       [  0.   ,   0.36 ,   0.   , ...,   6.423,  43.   , 334.   ]]), 'ytrain': array([[1],\n",
      "       [0],\n",
      "       [1],\n",
      "       ...,\n",
      "       [0],\n",
      "       [0],\n",
      "       [0]], dtype=uint8), 'ytest': array([[0],\n",
      "       [0],\n",
      "       [0],\n",
      "       ...,\n",
      "       [1],\n",
      "       [0],\n",
      "       [0]], dtype=uint8)}\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = loadmat('spamData.mat')\n",
    "# preview data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframes for visualising and simple processing\n",
    "X_train = pd.DataFrame(data['Xtrain'])\n",
    "X_test = pd.DataFrame(data['Xtest'])\n",
    "y_train = pd.DataFrame(data['ytrain'])\n",
    "y_test = pd.DataFrame(data['ytest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "One can try different preprocessing of the features. Consider the following separately:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) log-transform: \n",
    "Transform each feature using log(xij + 0.1) (assume natural log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "log_X_train = X_train.applymap(lambda x: np.log(x+0.1))\n",
    "log_X_test = X_test.applymap(lambda x: np.log(x+0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     1     2    3     4     5     6     7     8     9   ...   47     48  \\\n",
      "0  0.51  0.43  0.29  0.0  0.14  0.03  0.00  0.18  0.54  0.62  ...  0.0  0.012   \n",
      "1  0.00  0.00  1.44  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.0  0.000   \n",
      "2  0.16  0.32  0.65  0.0  0.32  0.00  0.16  0.00  0.00  0.49  ...  0.0  0.000   \n",
      "3  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.0  0.000   \n",
      "4  0.00  0.22  0.33  0.0  0.22  0.11  0.00  0.00  0.00  0.00  ...  0.0  0.019   \n",
      "\n",
      "      49   50     51     52     53     54     55      56  \n",
      "0  0.078  0.0  0.478  0.509  0.127  6.518  611.0  2340.0  \n",
      "1  0.247  0.0  0.000  0.000  0.000  2.380    8.0    50.0  \n",
      "2  0.000  0.0  0.773  0.080  0.080  6.586  132.0   955.0  \n",
      "3  0.000  0.0  0.000  0.000  0.000  2.333    5.0     7.0  \n",
      "4  0.253  0.0  0.000  0.000  0.000  2.068   11.0   395.0  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "         0         1         2         3         4         5         6   \\\n",
      "0 -0.494296 -0.634878 -0.941609 -2.302585 -1.427116 -2.040221 -2.302585   \n",
      "1 -2.302585 -2.302585  0.431782 -2.302585 -2.302585 -2.302585 -2.302585   \n",
      "2 -1.347074 -0.867501 -0.287682 -2.302585 -0.867501 -2.302585 -1.347074   \n",
      "3 -2.302585 -2.302585 -2.302585 -2.302585 -2.302585 -2.302585 -2.302585   \n",
      "4 -2.302585 -1.139434 -0.843970 -2.302585 -1.139434 -1.560648 -2.302585   \n",
      "\n",
      "         7         8         9   ...        47        48        49        50  \\\n",
      "0 -1.272966 -0.446287 -0.328504  ... -2.302585 -2.189256 -1.725972 -2.302585   \n",
      "1 -2.302585 -2.302585 -2.302585  ... -2.302585 -2.302585 -1.058430 -2.302585   \n",
      "2 -2.302585 -2.302585 -0.527633  ... -2.302585 -2.302585 -2.302585 -2.302585   \n",
      "3 -2.302585 -2.302585 -2.302585  ... -2.302585 -2.302585 -2.302585 -2.302585   \n",
      "4 -2.302585 -2.302585 -2.302585  ... -2.302585 -2.128632 -1.041287 -2.302585   \n",
      "\n",
      "         51        52        53        54        55        56  \n",
      "0 -0.548181 -0.495937 -1.482805  1.889793  6.415261  7.757949  \n",
      "1 -2.302585 -2.302585 -2.302585  0.908259  2.091864  3.914021  \n",
      "2 -0.135820 -1.714798 -1.714798  1.900016  4.883559  6.861816  \n",
      "3 -2.302585 -2.302585 -2.302585  0.889125  1.629241  1.960095  \n",
      "4 -2.302585 -2.302585 -2.302585  0.773805  2.406945  5.979139  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "# preview & compare\n",
    "print(X_train.head())\n",
    "print(log_X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b) binarization: \n",
    "Binarize features: I(xij > 0). In other words, if a feature is greater than 0, it’s simply set to 1. If it’s less than or equal to 0, it’s set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "bnrz_X_train = X_train.applymap(lambda x: 1 if x>0 else 0)\n",
    "bnrz_X_test = X_test.applymap(lambda x: 1 if x>0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     1     2    3     4     5     6     7     8     9   ...   47     48  \\\n",
      "0  0.51  0.43  0.29  0.0  0.14  0.03  0.00  0.18  0.54  0.62  ...  0.0  0.012   \n",
      "1  0.00  0.00  1.44  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.0  0.000   \n",
      "2  0.16  0.32  0.65  0.0  0.32  0.00  0.16  0.00  0.00  0.49  ...  0.0  0.000   \n",
      "3  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.0  0.000   \n",
      "4  0.00  0.22  0.33  0.0  0.22  0.11  0.00  0.00  0.00  0.00  ...  0.0  0.019   \n",
      "\n",
      "      49   50     51     52     53     54     55      56  \n",
      "0  0.078  0.0  0.478  0.509  0.127  6.518  611.0  2340.0  \n",
      "1  0.247  0.0  0.000  0.000  0.000  2.380    8.0    50.0  \n",
      "2  0.000  0.0  0.773  0.080  0.080  6.586  132.0   955.0  \n",
      "3  0.000  0.0  0.000  0.000  0.000  2.333    5.0     7.0  \n",
      "4  0.253  0.0  0.000  0.000  0.000  2.068   11.0   395.0  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "   0   1   2   3   4   5   6   7   8   9   ...  47  48  49  50  51  52  53  \\\n",
      "0   1   1   1   0   1   1   0   1   1   1  ...   0   1   1   0   1   1   1   \n",
      "1   0   0   1   0   0   0   0   0   0   0  ...   0   0   1   0   0   0   0   \n",
      "2   1   1   1   0   1   0   1   0   0   1  ...   0   0   0   0   1   1   1   \n",
      "3   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "4   0   1   1   0   1   1   0   0   0   0  ...   0   1   1   0   0   0   0   \n",
      "\n",
      "   54  55  56  \n",
      "0   1   1   1  \n",
      "1   1   1   1  \n",
      "2   1   1   1  \n",
      "3   1   1   1  \n",
      "4   1   1   1  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "# preview & compare\n",
    "print(X_train.head())\n",
    "print(bnrz_X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Beta-binomial Naive Bayes\n",
    "Fit a Beta-Binomial naive Bayes classifier on the binarized data from the Data Processing section.<br>\n",
    "Do not need to assume prior on class label (Use Maximum Likelihood). \n",
    "Assume a beta prior on the feature distribution B(a,a). Please refer to the PDF report for more information.\n",
    "The first portion contains workings to explain the class and functions combined later. You may jump to the [Actual Codes](#Actual-Codes) portion if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viewing col_ 0 by y:\n",
      "spam & x=1: 417\n",
      "spam & x=0: 773\n",
      "not spam & x=1: 268\n",
      "not spam & x=0: 1607\n"
     ]
    }
   ],
   "source": [
    "# testing out function to split values by class & binary feature\n",
    "# try for first column\n",
    "col_idx = 0\n",
    "print('viewing col_',col_idx,'by y:')\n",
    "print('spam & x=1:', bnrz_X_train.loc[(y_train[0]==1) & (bnrz_X_train[col_idx]==1),col_idx].count())\n",
    "print('spam & x=0:', bnrz_X_train.loc[(y_train[0]==1) & (bnrz_X_train[col_idx]==0),col_idx].count())\n",
    "print('not spam & x=1:', bnrz_X_train.loc[(y_train[0]==0) & (bnrz_X_train[col_idx]==1),col_idx].count())\n",
    "print('not spam & x=0:', bnrz_X_train.loc[(y_train[0]==0) & (bnrz_X_train[col_idx]==0),col_idx].count())\n",
    "# check values tally to total\n",
    "assert \\\n",
    "    bnrz_X_train.loc[(y_train[0]==1) & (bnrz_X_train[col_idx]==1),col_idx].count() \\\n",
    "    + bnrz_X_train.loc[(y_train[0]==1) & (bnrz_X_train[col_idx]==0),col_idx].count() \\\n",
    "    + bnrz_X_train.loc[(y_train[0]==0) & (bnrz_X_train[col_idx]==1),col_idx].count() \\\n",
    "    + bnrz_X_train.loc[(y_train[0]==0) & (bnrz_X_train[col_idx]==0),col_idx].count() \\\n",
    "    == len(bnrz_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9280575539568345\n"
     ]
    }
   ],
   "source": [
    "# create beta function\n",
    "def calculate_beta_pdf(a,b,pos,neg):\n",
    "    \"\"\"\n",
    "    Calculates the pdf of Beta(a,b) distribution\n",
    "    a : alpha parameter\n",
    "    b : beta parameter\n",
    "    pos : number of positive instances\n",
    "    neg: number of negative instances\n",
    "    \"\"\"\n",
    "    return (pos+a)/(pos+neg+a+b)\n",
    "\n",
    "# try function on first column x SPAM\n",
    "set_a = 100\n",
    "print(\n",
    "    calculate_beta_pdf(\n",
    "        a=set_a,\n",
    "        b=set_a, \n",
    "        pos=bnrz_X_train.loc[(y_train[0]==1) & (bnrz_X_train[col_idx]==1),col_idx].count(),\n",
    "        neg=bnrz_X_train.loc[(y_train[0]==1) & (bnrz_X_train[col_idx]==0),col_idx].count()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.37194244604316545, 0.6280575539568345)\n"
     ]
    }
   ],
   "source": [
    "def calculate_beta_pdf_per_feature(x_df, y_df, col_idx, y_class):\n",
    "    x_pos = calculate_beta_pdf(\n",
    "        a=set_a,\n",
    "        b=set_a, \n",
    "        pos=x_df.loc[(y_df[0]==y_class) & (x_df[col_idx]==1),col_idx].count(),\n",
    "        neg=x_df.loc[(y_df[0]==y_class) & (x_df[col_idx]==0),col_idx].count()\n",
    "    )\n",
    "    x_neg = 1-x_pos\n",
    "    return x_pos, x_neg\n",
    "\n",
    "# try function on first column x SPAM\n",
    "print(\n",
    "    calculate_beta_pdf_per_feature(\n",
    "        x_df=bnrz_X_train, \n",
    "        y_df=y_train,\n",
    "        col_idx=0,\n",
    "        y_class=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.371942</td>\n",
       "      <td>0.360432</td>\n",
       "      <td>0.595683</td>\n",
       "      <td>0.08777</td>\n",
       "      <td>0.610072</td>\n",
       "      <td>0.388489</td>\n",
       "      <td>0.438129</td>\n",
       "      <td>0.353957</td>\n",
       "      <td>0.328777</td>\n",
       "      <td>0.457554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079856</td>\n",
       "      <td>0.197122</td>\n",
       "      <td>0.627338</td>\n",
       "      <td>0.135252</td>\n",
       "      <td>0.78777</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.326619</td>\n",
       "      <td>0.928058</td>\n",
       "      <td>0.928058</td>\n",
       "      <td>0.928058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.628058</td>\n",
       "      <td>0.639568</td>\n",
       "      <td>0.404317</td>\n",
       "      <td>0.91223</td>\n",
       "      <td>0.389928</td>\n",
       "      <td>0.611511</td>\n",
       "      <td>0.561871</td>\n",
       "      <td>0.646043</td>\n",
       "      <td>0.671223</td>\n",
       "      <td>0.542446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920144</td>\n",
       "      <td>0.802878</td>\n",
       "      <td>0.372662</td>\n",
       "      <td>0.864748</td>\n",
       "      <td>0.21223</td>\n",
       "      <td>0.406475</td>\n",
       "      <td>0.673381</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>0.071942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2        3         4         5         6   \\\n",
       "1  0.371942  0.360432  0.595683  0.08777  0.610072  0.388489  0.438129   \n",
       "0  0.628058  0.639568  0.404317  0.91223  0.389928  0.611511  0.561871   \n",
       "\n",
       "         7         8         9   ...        47        48        49        50  \\\n",
       "1  0.353957  0.328777  0.457554  ...  0.079856  0.197122  0.627338  0.135252   \n",
       "0  0.646043  0.671223  0.542446  ...  0.920144  0.802878  0.372662  0.864748   \n",
       "\n",
       "        51        52        53        54        55        56  \n",
       "1  0.78777  0.593525  0.326619  0.928058  0.928058  0.928058  \n",
       "0  0.21223  0.406475  0.673381  0.071942  0.071942  0.071942  \n",
       "\n",
       "[2 rows x 57 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_cls_data_from_train(x_train, y_train, y_class):\n",
    "    cls_data = pd.DataFrame()\n",
    "\n",
    "    for col_idx in range(0, x_train.shape[1]):\n",
    "        cls_data[col_idx] = list(calculate_beta_pdf_per_feature(\n",
    "            x_df=x_train, y_df=y_train, col_idx=col_idx, y_class=y_class\n",
    "        ))\n",
    "\n",
    "    cls_data.index = [1,0]\n",
    "\n",
    "    return cls_data\n",
    "\n",
    "# preview & check\n",
    "create_cls_data_from_train(bnrz_X_train, y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 1, 1]\n",
      "14.583333333333334\n"
     ]
    }
   ],
   "source": [
    "pos_cls_data = create_cls_data_from_train(bnrz_X_train, y_train, 1)\n",
    "pos_class_mle = y_train.sum()/len(y_train)\n",
    "pos_prob = []\n",
    "\n",
    "neg_cls_data = create_cls_data_from_train(bnrz_X_train, y_train, 0)\n",
    "neg_class_mle = 1-pos_class_mle\n",
    "neg_prob = []\n",
    "\n",
    "pred = []\n",
    "\n",
    "# loop through each row\n",
    "for row_idx in range(0, bnrz_X_test.shape[0]):\n",
    "    \n",
    "    row = bnrz_X_test.loc[row_idx,:]\n",
    "    pos_p = pos_class_mle[0]\n",
    "    neg_p = neg_class_mle[0]\n",
    "    \n",
    "    # loop through each feature\n",
    "    for col_idx in range(0, bnrz_X_test.shape[1]):\n",
    "        x_val = row[col_idx]\n",
    "        # prob y is spam\n",
    "        pos_p *= pos_cls_data.loc[x_val, col_idx]\n",
    "        # prob y is not spam\n",
    "        neg_p *= neg_cls_data.loc[x_val, col_idx]\n",
    "        \n",
    "#     print('pos_p', pos_p)\n",
    "#     print('neg_p', neg_p)\n",
    "    \n",
    "    # append prediction\n",
    "    if pos_p > neg_p:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "    \n",
    "    # optional storing of probabilites\n",
    "    pos_prob.append(pos_p)\n",
    "    neg_prob.append(neg_p)\n",
    "\n",
    "def calculate_error(pred, actual):\n",
    "    return (sum([1 if int(p)!=int(a) else 0 for p,a in zip(pred,actual)])/len(pred))*100\n",
    "\n",
    "print(pred[1:10])\n",
    "print(calculate_error(pred, y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaBinomialClassifier():\n",
    "\n",
    "    def __init__(self, a, b, x_1, y_1, x_2, y_2):\n",
    "        \"\"\"        \n",
    "        a : int \n",
    "            alpha parameter\n",
    "        b : int \n",
    "            beta parameter\n",
    "        x_1: pandas DataFrame\n",
    "            train features\n",
    "        y_1: pandas DataFrame\n",
    "            train target values\n",
    "        x_2: pandas DataFrame\n",
    "            test features\n",
    "        y_2: pandas DataFrame\n",
    "            test target values\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.bnrz_X_train = x_1\n",
    "        self.y_train = y_1\n",
    "        self.bnrz_X_test = x_2\n",
    "        self.y_test = y_2\n",
    "        \n",
    "    # functions\n",
    "    def calculate_beta_pdf(self, pos,neg):\n",
    "        \"\"\"\n",
    "        Calculates the pdf of Beta(a,b) distribution\n",
    "        pos : number of positive instances\n",
    "        neg: number of negative instances\n",
    "        \"\"\"\n",
    "        return (pos+self.a)/(pos+neg+self.a+self.b)\n",
    "\n",
    "    def calculate_error(self, pred, actual):\n",
    "        \"\"\"\n",
    "        Calculate percentage of wrong classification\n",
    "        \"\"\"\n",
    "        return (sum([1 if int(p)!=int(a) else 0 for p,a in zip(pred,actual)])/len(pred))*100\n",
    "    \n",
    "    # steps\n",
    "    def calculate_beta_pdf_per_feature(self, x_df, y_df, col_idx, y_class):\n",
    "        \"\"\"\n",
    "        Calculate one feature value per y_class\n",
    "        \"\"\"\n",
    "        x_pos = self.calculate_beta_pdf(\n",
    "            pos=x_df.loc[(y_df[0]==y_class) & (x_df[col_idx]==1),col_idx].count(),\n",
    "            neg=x_df.loc[(y_df[0]==y_class) & (x_df[col_idx]==0),col_idx].count()\n",
    "        )\n",
    "        x_neg = 1-x_pos\n",
    "        return x_pos, x_neg\n",
    "    \n",
    "    def create_cls_data_from_train(self, x_train, y_train, y_class):\n",
    "        \"\"\"\n",
    "        Calculate all feature values based on train data per y_class\n",
    "        and store into dataframe\n",
    "        \"\"\"\n",
    "        cls_data = pd.DataFrame()\n",
    "\n",
    "        for col_idx in range(0, x_train.shape[1]):\n",
    "            cls_data[col_idx] = list(self.calculate_beta_pdf_per_feature(\n",
    "                x_df=x_train, y_df=y_train, col_idx=col_idx, y_class=y_class\n",
    "            ))\n",
    "\n",
    "        cls_data.index = [1,0]\n",
    "\n",
    "        return cls_data\n",
    "    \n",
    "    def predict(self, x_to_pred, pos_cls_data, neg_cls_data, pos_class_mle, neg_class_mle):\n",
    "        \"\"\"\n",
    "        Predict on new data based on calculate feature information by class\n",
    "        \"\"\"\n",
    "\n",
    "        pred = []\n",
    "\n",
    "        # loop through each row\n",
    "        for row_idx in range(0, x_to_pred.shape[0]):\n",
    "\n",
    "            row = x_to_pred.loc[row_idx,:]\n",
    "            pos_p = pos_class_mle[0]\n",
    "            neg_p = neg_class_mle[0]\n",
    "\n",
    "            # loop through each feature\n",
    "            for col_idx in range(0, x_to_pred.shape[1]):\n",
    "                x_val = row[col_idx]\n",
    "                pos_p *= pos_cls_data.loc[x_val, col_idx]\n",
    "                neg_p *= neg_cls_data.loc[x_val, col_idx]\n",
    "\n",
    "            # append prediction\n",
    "            if pos_p > neg_p:\n",
    "                pred.append(1)\n",
    "            else:\n",
    "                pred.append(0)\n",
    "            \n",
    "        return pred\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run pipeline\n",
    "        \"\"\"\n",
    "        # class mle\n",
    "        pos_class_mle = self.y_train.sum()/len(self.y_train)\n",
    "        neg_class_mle = 1-pos_class_mle\n",
    "        # feature calculations\n",
    "        pos_cls_data = self.create_cls_data_from_train(self.bnrz_X_train, self.y_train, 1)\n",
    "        neg_cls_data = self.create_cls_data_from_train(self.bnrz_X_train, self.y_train, 0)\n",
    "        # get predictions\n",
    "        train_pred = self.predict(self.bnrz_X_train, pos_cls_data, neg_cls_data, pos_class_mle, neg_class_mle)\n",
    "        test_pred = self.predict(self.bnrz_X_test, pos_cls_data, neg_cls_data, pos_class_mle, neg_class_mle)\n",
    "        # score and return results\n",
    "        return self.calculate_error(train_pred, self.y_train[0]), \\\n",
    "               self.calculate_error(test_pred, self.y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.605220228384992, 14.583333333333334)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trial on one round\n",
    "set_a = 100\n",
    "bbc = BetaBinomialClassifier(set_a, set_a, bnrz_X_train, y_train, bnrz_X_test, y_test)\n",
    "bbc.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 (a) \n",
    "##### Plot errors across alpha = {0, 0.5, 1, 1.5, 2, · · · , 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialise empty list for storage\n",
    "list_of_alphas = list(np.arange(0, 100.5, 0.5))\n",
    "train_error_list = []\n",
    "test_error_list = []\n",
    "\n",
    "# loop over alpha values\n",
    "for set_a in list_of_alphas:\n",
    "    bbc = BetaBinomialClassifier(set_a, set_a, bnrz_X_train, y_train, bnrz_X_test, y_test)\n",
    "    train_error, test_error = bbc.run()\n",
    "    train_error_list.append(train_error)\n",
    "    test_error_list.append(test_error)\n",
    "    \n",
    "# plot chart\n",
    "plt.plot(list_of_alphas, train_error_list, label='Train Error')\n",
    "plt.plot(list_of_alphas, test_error_list, label='Test Error')\n",
    "plt.ylabel('Error Rate %')\n",
    "plt.xlabel('Alpha Value')\n",
    "plt.title('Training and Testing Data Error Rates vs Alpha Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 (b) \n",
    "##### What do you observe about the training and test errors as alpha changes?\n",
    "Referring to the plot above, the errors increase as alpha increases, with a sharp jump at around alpha=80. For every value of alpha, the test error rates are always higher than the train error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 (c)\n",
    "##### Training and testing error rates for alpha = 1, 10 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error for alpha=1.0 is:  10.962479608482871%\n",
      "The testing error for alpha=1.0 is:  11.393229166666668%\n",
      "The training error for alpha=10.0 is:  11.582381729200652%\n",
      "The testing error for alpha=10.0 is:  12.434895833333332%\n",
      "The training error for alpha=100.0 is:  13.605220228384992%\n",
      "The testing error for alpha=100.0 is:  14.583333333333334%\n"
     ]
    }
   ],
   "source": [
    "for val in [1,10,100]:\n",
    "    idx = list_of_alphas.index(val)\n",
    "    print('The training error for alpha='+str(list_of_alphas[idx])+' is: ', str(train_error_list[idx])+'%')\n",
    "    print('The testing error for alpha='+str(list_of_alphas[idx])+' is: ', str(test_error_list[idx])+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Gaussian Naive Bayes\n",
    "Fit a Gaussian naive Bayes classifier on the log-transformed data from the Data Processing section. Class label prior is estimated using ML as per before. \n",
    "Use maximum likelihood to estimate the class conditional mean and variance of each feature and use ML estimates as a plug-in estimator for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianClassifier():\n",
    "\n",
    "    def __init__(self, x_1, y_1, x_2, y_2):\n",
    "        \"\"\"        \n",
    "        x_1: pandas DataFrame\n",
    "            train features\n",
    "        y_1: pandas DataFrame\n",
    "            train target values\n",
    "        x_2: pandas DataFrame\n",
    "            test features\n",
    "        y_2: pandas DataFrame\n",
    "            test target values\n",
    "        \"\"\"\n",
    "        self.log_X_train = x_1\n",
    "        self.y_train = y_1\n",
    "        self.log_X_test = x_2\n",
    "        self.y_test = y_2\n",
    "        \n",
    "    # functions\n",
    "    \n",
    "    def calculate_mean(self, vector):\n",
    "        \"\"\"\n",
    "        ML estimation of mean\n",
    "        \"\"\"\n",
    "        return sum(vector)/len(vector)\n",
    "    \n",
    "    def calculate_variance(self, vector, mean):\n",
    "        \"\"\"\n",
    "        ML estimation of variance\n",
    "        \"\"\"\n",
    "        return sum([(i-mean)*(i-mean) for i in vector])/len(vector)\n",
    "    \n",
    "    def calculate_gaussian_pdf(self, x, mean, sig2):\n",
    "        \"\"\"\n",
    "        Calculates the pdf of Normal(mean,sig2) distribution\n",
    "        x: actual observation value\n",
    "        mean: mean of observations\n",
    "        sig2: variance of observations\n",
    "        \"\"\"\n",
    "        return (1/((2*np.pi*(sig2))**0.5))*np.exp((-0.5 * ((x - mean)*(x - mean)))/(sig2))\n",
    "\n",
    "    def calculate_error(self, pred, actual):\n",
    "        \"\"\"\n",
    "        Calculate percentage of wrong classification\n",
    "        \"\"\"\n",
    "        return (sum([1 if int(p)!=int(a) else 0 for p,a in zip(pred,actual)])/len(pred))*100\n",
    "    \n",
    "    # steps\n",
    "    def create_cls_data_from_train(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Calculate feature values based on train data\n",
    "        and store into separate lists\n",
    "        \"\"\"\n",
    "        pos_mean_list = []\n",
    "        pos_var_list = []\n",
    "        neg_mean_list = []\n",
    "        neg_var_list = []\n",
    "\n",
    "        for col_idx in range(0, x_train.shape[1]):\n",
    "            pos_mean = self.calculate_mean(x_train.loc[y_train[0]==1, col_idx])\n",
    "            pos_var = self.calculate_variance(x_train.loc[y_train[0]==1, col_idx], pos_mean)\n",
    "            neg_mean = self.calculate_mean(x_train.loc[y_train[0]==0, col_idx])\n",
    "            neg_var = self.calculate_variance(x_train.loc[y_train[0]==0, col_idx], neg_mean)\n",
    "            pos_mean_list.append(pos_mean)\n",
    "            pos_var_list.append(pos_var)\n",
    "            neg_mean_list.append(neg_mean)\n",
    "            neg_var_list.append(neg_var)\n",
    "            \n",
    "        return pos_mean_list, pos_var_list, neg_mean_list, neg_var_list\n",
    "    \n",
    "    def predict(self, x_to_pred, \n",
    "                pos_mean_list, pos_var_list, \n",
    "                neg_mean_list, neg_var_list, \n",
    "                pos_class_mle, neg_class_mle\n",
    "               ):\n",
    "        \"\"\"\n",
    "        Predict on new data based on calculate feature information by class\n",
    "        \"\"\"\n",
    "\n",
    "        pred = []\n",
    "\n",
    "        # loop through each row\n",
    "        for row_idx in range(0, x_to_pred.shape[0]):\n",
    "\n",
    "            row = x_to_pred.loc[row_idx,:]\n",
    "            pos_p = np.log(pos_class_mle[0])\n",
    "            neg_p = np.log(neg_class_mle[0])\n",
    "\n",
    "            # loop through each feature\n",
    "            for col_idx in range(0, x_to_pred.shape[1]):\n",
    "                x_val = row[col_idx]\n",
    "                pos_p += np.log(self.calculate_gaussian_pdf(x_val, pos_mean_list[col_idx], pos_var_list[col_idx]))\n",
    "                neg_p += np.log(self.calculate_gaussian_pdf(x_val, neg_mean_list[col_idx], neg_var_list[col_idx]))\n",
    "\n",
    "            # append prediction\n",
    "            if pos_p > neg_p:\n",
    "                pred.append(1)\n",
    "            else:\n",
    "                pred.append(0)\n",
    "            \n",
    "        return pred\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run pipeline\n",
    "        \"\"\"\n",
    "        # class mle\n",
    "        pos_class_mle = self.y_train.sum()/len(self.y_train)\n",
    "        neg_class_mle = 1-pos_class_mle\n",
    "        # feature calculations\n",
    "        pos_mean_list, pos_var_list, neg_mean_list, neg_var_list = self.create_cls_data_from_train(self.log_X_train, self.y_train)\n",
    "        # get predictions\n",
    "        train_pred = self.predict(self.log_X_train, pos_mean_list, pos_var_list, neg_mean_list, neg_var_list, pos_class_mle, neg_class_mle)\n",
    "        test_pred = self.predict(self.log_X_test, pos_mean_list, pos_var_list, neg_mean_list, neg_var_list, pos_class_mle, neg_class_mle)\n",
    "        # score and return results\n",
    "        return self.calculate_error(train_pred, self.y_train[0]), \\\n",
    "               self.calculate_error(test_pred, self.y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 (a)\n",
    "##### Training and testing error rates for the log-transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\effbl\\anaconda3\\envs\\ee5907_ca1\\lib\\site-packages\\ipykernel_launcher.py:86: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\effbl\\anaconda3\\envs\\ee5907_ca1\\lib\\site-packages\\ipykernel_launcher.py:87: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16.6721044045677, 18.359375)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trial on one round\n",
    "gsc = GaussianClassifier(log_X_train, y_train, log_X_test, y_test)\n",
    "gsc.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Logistic Regression\n",
    "For the log-transformed data, fit a logistic regression model with l2 regularization\n",
    "For each regularization parameter value, fit the logistic regression model on the training\n",
    "data and compute its error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self, lmd, ms, x_1, y_1, x_2, y_2):\n",
    "        \"\"\"    \n",
    "        lambda: int\n",
    "            lambda parameter\n",
    "        max_steps: int\n",
    "            max number of steps\n",
    "        x_1: pandas DataFrame\n",
    "            train features\n",
    "        y_1: pandas DataFrame\n",
    "            train target values\n",
    "        x_2: pandas DataFrame\n",
    "            test features\n",
    "        y_2: pandas DataFrame\n",
    "            test target values\n",
    "        \"\"\"\n",
    "        self.lmda = lmd\n",
    "        self.max_steps = ms\n",
    "        self.log_X_train = x_1\n",
    "        self.y_train = y_1\n",
    "        self.log_X_test = x_2\n",
    "        self.y_test = y_2\n",
    "    \n",
    "    # functions\n",
    "    def calculate_logistic_pdf(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the pdf of sigma(x) distribution\n",
    "        x: actual observation value\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def calculate_error(self, pred, actual):\n",
    "        \"\"\"\n",
    "        Calculate percentage of wrong classification\n",
    "        \"\"\"\n",
    "        return (sum([1 if int(p)!=int(a) else 0 for p,a in zip(pred,actual)])/len(pred))*100\n",
    "    \n",
    "    # steps\n",
    "    def find_w_from_train(self, x_train, y_train, lmd, max_steps=300):\n",
    "        \"\"\"\n",
    "        Search for w that using Newton's Method and l2 regularisation\n",
    "        \"\"\"\n",
    "        # add first x term to be bias\n",
    "        # we now have N X (D+1) matrix\n",
    "        x_train['bias'] = 1\n",
    "        \n",
    "        # We convert pandas DataFrames into numpy arrays for easier matrix multiplication\n",
    "        X = x_train.values\n",
    "        X_t = np.transpose(X)\n",
    "        # y is N x 1 vector\n",
    "        y = y_train.values\n",
    "        \n",
    "        # initialise parameters\n",
    "        # w is a (D+1) X 1 vector\n",
    "        w = np.zeros((x_train.shape[1], 1))\n",
    "        w_t = np.transpose(w)\n",
    "        # store error rates into a list\n",
    "        error_rate_list = []\n",
    "        \n",
    "        # iterate until convergence\n",
    "        for step in range(0, max_steps):\n",
    "            \n",
    "            ##### calculate mu parameter which is Nx1 #####\n",
    "            mu = []\n",
    "            for row_idx in range(0, X.shape[0]):\n",
    "                # Dimension: (1 x (D+1)) x ((D+1) x 1) returns a scalar\n",
    "                row_mu = self.calculate_logistic_pdf(np.dot(w_t, X[row_idx]))\n",
    "                mu.append(row_mu)\n",
    "            # for consistency, store all lists as numpy arrays\n",
    "            mu = np.array(mu)\n",
    "\n",
    "            ##### calculate gradient matrix #####\n",
    "            # Dimension: ((D+1) x N) x (Nx1)\n",
    "            g = np.dot(X_t, (mu-y))\n",
    "            # format regularisation term without bias (vector with first term as 0)\n",
    "            regu_w = w.copy()\n",
    "            regu_w[0] = 0\n",
    "            # calculate regularised g\n",
    "            g_reg = g + lmd*regu_w\n",
    "            \n",
    "            ##### calculate hessian matrix #####\n",
    "            mu_diag = np.array([i*(1-i) for i in mu])\n",
    "            S = np.diag(mu_diag.flatten())\n",
    "            H = np.dot(np.dot(X_t, S), X)\n",
    "            # format regularisation term without bias (identity matrix with first,first term as 0)\n",
    "            regu_H = np.eye(X_train.shape[1]+1)\n",
    "            regu_H[0,0]=0\n",
    "            # calculate regularised H\n",
    "            H_reg = H + lmd*regu_H\n",
    "            \n",
    "            ##### solve for dk #####\n",
    "            # Dimension: ((D+1) X (D+1)) x ((D+1) X 1)\n",
    "            d = -np.dot(np.linalg.inv(H_reg), g_reg)\n",
    "            \n",
    "            ##### update parameter #####\n",
    "            # Dimension: (D+1) X 1)\n",
    "            w += d\n",
    "            \n",
    "        return w\n",
    "\n",
    "    def predict(self, x_to_pred, w):\n",
    "        \"\"\"\n",
    "        Predict on new data based on calculate feature information by class\n",
    "        \"\"\"\n",
    "        # add bias term\n",
    "        x_to_pred['bias'] = 1\n",
    "        # format into numpy arrays for easier matrix multiplication\n",
    "        X = x_to_pred.values\n",
    "        # calculate coefficients\n",
    "        # Dimension: (N x (D+1)) x ((D+1) x 1) = N x 1\n",
    "        log_odds = np.dot(X,w)\n",
    "        pred = [1 if i>0 else 0 for i in log_odds]\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def run(self):\n",
    "        # get w based on feature values\n",
    "        fitted_w = self.find_w_from_train(self.log_X_train, self.y_train, self.lmda, self.max_steps)\n",
    "        # get predictions\n",
    "        train_pred = self.predict(self.log_X_train, fitted_w)\n",
    "        test_pred = self.predict(self.log_X_test, fitted_w)\n",
    "        # score and return results\n",
    "        return self.calculate_error(train_pred, self.y_train[0]), \\\n",
    "               self.calculate_error(test_pred, self.y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.220228384991843, 5.989583333333334)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trial on one round\n",
    "lmda = 1\n",
    "max_steps = 100\n",
    "lgr = LogisticRegression(lmda, max_steps, log_X_train, y_train, log_X_test, y_test)\n",
    "lgr.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 (a)\n",
    "##### Plot errors across lambda = {1, 2, · · · , 9, 10, 15, 20, · · · , 95, 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise empty list for storage\n",
    "list_of_lambdas = list(np.arange(1, 10, 1))\n",
    "list_of_lambdas.extend(list(np.arange(10, 105, 5)))\n",
    "max_steps = 100\n",
    "train_error_list = []\n",
    "test_error_list = []\n",
    "\n",
    "# loop over alpha values\n",
    "for set_l in list_of_alphas:\n",
    "    lgr = LogisticRegression(lmda, max_steps, log_X_train, y_train, log_X_test, y_test)\n",
    "    train_error, test_error = lgr.run()\n",
    "    train_error_list.append(train_error)\n",
    "    test_error_list.append(test_error)\n",
    "    \n",
    "# plot chart\n",
    "plt.plot(list_of_lambdas, train_error_list, label='Train Error')\n",
    "plt.plot(list_of_lambdas, test_error_list, label='Test Error')\n",
    "plt.ylabel('Error Rate %')\n",
    "plt.xlabel('Lambda Value')\n",
    "plt.title('Training and Testing Data Error Rates vs Lambda Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 (b) \n",
    "##### What do you observe about the training and test errors as lambda changes?\n",
    "Referring to the plot above, at the beginning, increasing lambda reduces test error rate with little effect on train error rates. From around lambda=8 onwards, as a whole, the errors increase as lambda increases, with a steep increase when lambda is 8-20 for test error. For every value of lambda, the test error rates are always higher than the train error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 (c)\n",
    "##### Training and testing error rates for lambda = 1, 10 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in [1,10,100]:\n",
    "    idx = list_of_lambdas.index(val)\n",
    "    print('The training error for lambda='+str(list_of_lambdas[idx])+' is: ', str(train_error_list[idx])+'%')\n",
    "    print('The testing error for lambda='+str(list_of_lambdas[idx])+' is: ', str(test_error_list[idx])+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. K-Nearest Neighbors\n",
    "For the log-transformed data, implement a KNN classifier (see week 5 lecture notes on “Nonparametric\n",
    "Classification”). Use the Euclidean distance to measure distance between neighbors. For each value of K, compute the training and test error rates (i.e., percentage of emails classified wrongly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec_1, vec_2):\n",
    "    \"\"\"\n",
    "    Calculates the euclidean distance between two vectors vec_1 and vec_2\n",
    "    \"\"\"\n",
    "    return sum([(one-two)**2 in zip(vec_1, vec_2)])**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Survey\n",
    "##### Please give an estimate of how much time you spent on this assignment.\n",
    "Took around 30 hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
